<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>openmmdetaction</title>
      <link href="/2022/07/01/openmmlab/"/>
      <url>/2022/07/01/openmmlab/</url>
      
        <content type="html"><![CDATA[<h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>图像分类：</p><ol><li>将图像切分成多块</li><li>用分类算法对每个图像块进行预测</li><li>检测结果=（分类结果，图像位置）</li></ol><p>问题：图像分块过于粗糙，无法检测分块边界上的物体</p><p><strong>改进</strong>：</p><ol><li>使用重叠的窗口，覆盖更多可能出现物体的位置</li><li>用分类算法检测每个图像块</li><li>检测结果=(分类结果，滑窗位置)</li></ol><p>问题：滑窗边界与物体精确边界有偏差</p><p><strong>改进</strong>：分类的同时预测物体的精确位置 –<strong>边界框回归</strong></p><p><strong>问题</strong>：不同物体大小不同，长宽比不同</p><p><strong>解决方案：</strong></p><ol><li><p>使用大小不同，长宽比不同的滑窗（yolo里面是9种）</p><p><img src="/2022/07/01/openmmlab/image-20220701140554357.png"></p></li><li><p>将图像缩放到不同大小，构建图像金字塔，相同大小的窗口在不同尺寸的图像上可以检测不同尺寸的物体</p></li></ol><p><img src="/2022/07/01/openmmlab/image-20220701140331584.png"></p><p>滑窗 = 空间上的密集预测</p><p>考虑800×600的图像，使用80×60的窗，步长10像素滑动</p><p>需要在4800个窗上进行分类预测</p><p>×每个位置5个尺度的窗</p><p>×每个尺度3个长宽比</p><p>≈检测一张图像需要完成数万次图像分类预测 （<font color="red">难以实现实时检测</font>）</p><p><strong>分析：</strong>大量窗口都落在不包含物体的边界区域。可以先通过简单快速的方法找出可能含物体的区域。</p><p><strong>Selective Search算法：</strong></p><p>使用贪心算法，将空间相邻且特征相似的图像块逐步合并到一起，形成可能包含物体的区域，<strong>称为提议区域或提议框</strong>。</p><p><img src="/2022/07/01/openmmlab/image-20220701150042983.png"></p><h2 id="基于区域的检测方法R-CNN"><a href="#基于区域的检测方法R-CNN" class="headerlink" title="基于区域的检测方法R-CNN"></a>基于区域的检测方法R-CNN</h2><p>两阶段方法：</p><ol><li><p>可学习部分：卷积神经网络的主干，分类器，回归模型</p><p>对于提议框P和标注框B</p><ul><li><p>如果二者重叠较大</p><p>分类目标值 = B的类别标注</p><p>回归目标值 = 编码后的偏差值</p></li><li><p>如果二者重叠较小</p><p>分类目标值 = 背景</p><p>回归分支不计算LOSS</p></li></ul><p><img src="/2022/07/01/openmmlab/image-20220705162437266.png"></p></li><li><p>IOU 定义为两个矩形框交集面积与并集面积的比值</p></li><li><p>非极大值抑制（NMS）</p><p>检测算法有时会针对单个物体给出多个相近的检测框，并在所有重叠框中保留置信度最高的一个</p><p>算法过程：</p><p><strong>输入：</strong>检测器产生的一系列检测框P = {P<del>1</del>，……，P<del>n</del>}以及对应置信度s = {s<del>1</del>，……，s<del>n</del>}，IoU阈值为t</p><p><strong>步骤：</strong> 1.初始化结果集 R 为空集</p><p>​            2.重复至 P 为空集</p><ul><li>找出P中置信度最大的框Pi并加入R</li><li>从P中删除Pi以及与Pi交并比大于t的框</li></ul></li></ol><p>​        <strong>输出：</strong>结果集为R</p><h2 id="历史上的RCNN"><a href="#历史上的RCNN" class="headerlink" title="历史上的RCNN"></a>历史上的RCNN</h2><ul><li>卷积网络使用AlexNet</li><li>使用Selective Search产生提议框</li><li>非端到端训练，先训练卷积网络的主干部分，再基于卷积网络的特征训练SVM分类器</li><li>非多任务学习，先训练分类模型，再边界框回归单独训练</li><li>根据配置不同，单图推理几秒~几十秒</li></ul><h2 id="两阶段方法的演进（2014-2017）"><a href="#两阶段方法的演进（2014-2017）" class="headerlink" title="两阶段方法的演进（2014-2017）"></a>两阶段方法的演进（2014-2017）</h2><p><strong>Fast RCNN ：</strong>提出了Rol Pooling方法，把区域从图像移动到特征图上，大幅降低了计算量。</p><p>具体流程：</p><p><img src="/2022/07/01/openmmlab/image-20220706185621684.png"></p><ol><li>将原图的提议框按照几何比例缩放到特征图上</li><li>利用RoI Pooling,将不同尺寸的特征图变换到相同大小，对于每个提议框，可以的得到固定尺寸的特征图。</li><li>基于相同尺寸的特征图进行分类和回归</li></ol><p>Fast RCNN虽然速度上远超RCNN，但产生提议框仍依赖外部算法，成为速度瓶颈</p><p><strong>解决方案：</strong>使用卷积网络产生提议框，并与检测器共享主干网络结构</p><p><strong>Faster RCNN：</strong>提出了RPN网络，用于替换传统方法，产生区域提议，进一步提高效率。</p><p>使用滑窗的思路进行空间密集预测，并使用轻量网络（例如单层3×3卷积）作为检测头，降低计算量。</p><p>引入锚框的概念来处理不同大小的物体，不同的锚框共享特征，多个检测头为同一卷积层的不同通道。</p><p>RPN产生区域提议：</p><ol><li>主干网络产生特征图</li><li>不同的检测头产生区域提议(分类+边界框回归</li><li>集合所有提议框，做NMS</li></ol><p><img src="/2022/07/01/openmmlab/image-20220706212009207.png"></p><p>训练过程：</p><ul><li><p>Faster RCNN RPN Fast RCNN</p></li><li><p>每部分产生分类和回归oss</p></li><li><p>两者共同回传梯度、共同优化</p><p><img src="/2022/07/01/openmmlab/image-20220706212344427.png"></p></li></ul><p><strong>Mask RCNN：</strong>提出了ROIAlign算法替换ROIPooling，加入用于实例分割的分支。</p><p><strong>问题：</strong>RoI Pooling对非整数边界框取整，产生位置偏差</p><p><strong>思路：</strong>为保留空间精度，使用“非整数坐标”的特征</p><p>方法：插值-&gt;RoI Align （双线性插值）</p><p>算法流程：</p><ol><li>将提议区域切成固定数目的格子，例如7×7</li><li>在每个格子中，均匀选取若干采样点，如2×2=4个</li><li>通过插值方法得到每个采样点处的精确特征</li><li>所有采样点做Pooling得到输出结果</li></ol><p>单级特征图的问题：</p><p>高层次特诊空间降采样率比较大-&gt;小物体信息丢失</p><p><strong>解决思路：</strong></p><ol><li>基于低一层特侦图预测 -&gt;低层特征语义信息薄弱</li><li>图像金字塔 -&gt;低效率</li></ol><p>通过融合高＋低层次的特征 -&gt;特征金字塔网络（FPN）</p><h2 id="MMDetection"><a href="#MMDetection" class="headerlink" title="MMDetection"></a>MMDetection</h2><p><strong>mmdet：</strong></p><ul><li>API：包括训练，推理，测试的高层次API</li><li>core：anchor,bbox,mask等模块的实现</li><li>datasets：数据集支持，数据预处理与数据增强</li><li>mdels：检测模型的实现</li><li>utils：辅助工具</li></ul>]]></content>
      
      
      <categories>
          
          <category> openmmlab </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>yolov3说明</title>
      <link href="/2022/06/23/yolov3-shuo-ming/"/>
      <url>/2022/06/23/yolov3-shuo-ming/</url>
      
        <content type="html"><![CDATA[<ul><li><p>yolo是根据中心点，w和h  （速度快，互不影响）</p></li><li><p>传统目标检测：左下角和右下角的坐标</p></li></ul><p>yolo将物体分为了9种：大中小—正方形 长矩形 竖矩形</p><p>首先把图片等分，然后根据锚点画框，最后通过IOU来判断框。</p><p>偏移量：通过中心点的坐标去数格子，坐标除以格子的宽度可以得到格子的位置 再分离整数部分和小数部分，小数部分就是偏移量。（求pw,ph,分类）【对神经网络的输出取指数，最后取对数】</p><p>1*1卷积的目的是降通道，例如（32，32，10）通过（1，1，10）生成（32，32，1）的特征图</p><p>3*3卷积的目的是像素融合并进行降通道</p><p>下采样原理:图像尺寸为M<em>N。对其进行s倍下采样，即得到（M/s）</em>(N/s)尺寸的分辨率图像，s需为M和N的公约数，对于矩阵形式图像的含义即为将原始图像s*s窗口内的图像编程为一个像素，这个像素点的值就是窗口内所有像素的均值。在YOLOv3的网络结构中并未采用最大值池化或平均值池化方法进行降采样，而是采用步长为2的卷积来进行降采样。</p><p>上采样原理:图像放大几乎都是采用内插值方法，即在原有图像的基础上在各个像素之间采用合适的插值算法插入新的元素。</p><p>在YOLOv3的网络结构中共图像的像素进行了5次下采样，每次采样步长为2，所以输入图像的大小需要为32的倍数。</p><p>concat层：张量拼接 将darknet中间层和后面的某一层的上采样进行拼接（特征有前后关系，深度不同的情况下）</p><p><img src="/2022/06/23/yolov3-shuo-ming/image-20220609191045044.png"></p><p><span class="github-emoji"><span>😂</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f602.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></p>]]></content>
      
      
      <categories>
          
          <category> yolo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
